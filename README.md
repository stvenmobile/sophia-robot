# ü§ñ Sophia ‚Äî A Voice Assistant Robot for Kids

[![Jetson Orin Nano](https://img.shields.io/badge/Platform-Jetson%20Orin%20Nano-green)](#)
[![Status](https://img.shields.io/badge/Status-Prototype-orange)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

Sophia is a friendly **AI voice companion for kids**, designed to answer questions, tutor, and tell stories ‚Äî fully **local** on a Jetson Orin Nano for privacy and low latency.

---

## ‚ú® What works today (Phase 1 MVP)

- **TTS**: Piper running in Docker with two voices (Amy default, Kristin optional)
- **ASR**:
  - `assistant.py` ‚Üí OpenAI Whisper (baseline)
  - `assistant_fw.py` ‚Üí faster-whisper (GPU-optimized) with CLI flags
- **Tiny RAG ‚Üí Local LLM**: sends the transcript + small context to a local LLM endpoint (`LLAMA_URL`)
- **Quick single-turn test**: `ask_once.py` (record ‚Üí transcribe ‚Üí LLM ‚Üí speak)
- **Wake word (optional)**: `wakeword_assistant.py` using Porcupine (‚Äúcomputer‚Äù) for hands-free

---

## üóÇ Repo layout (relevant bits)
```text
tools/
‚îî‚îÄ‚îÄ assistant/
    ‚îú‚îÄ‚îÄ assets/
    ‚îÇ   ‚îú‚îÄ‚îÄ bip.wav
    ‚îÇ   ‚îî‚îÄ‚îÄ bip2.wav
    ‚îú‚îÄ‚îÄ assistant.py            # Whisper baseline
    ‚îú‚îÄ‚îÄ assistant_fw.py         # faster-whisper + CLI
    ‚îú‚îÄ‚îÄ ask_once.py             # one-shot Q‚ÜíA test
    ‚îú‚îÄ‚îÄ wakeword_assistant.py   # wake-word ("computer") ‚Üí Q‚ÜíA loop
    ‚îî‚îÄ‚îÄ docker/
        ‚îî‚îÄ‚îÄ piper/
            ‚îî‚îÄ‚îÄ Dockerfile      # Piper build (with espeak-ng data fix)
```


---

## üß± Prerequisites

- Jetson Orin Nano with JetPack (CUDA/cuDNN installed)
- Python 3.10+
- ALSA utils on host: `sudo apt-get install -y alsa-utils`
- Docker installed and working
- A local LLM server reachable via `LLAMA_URL` (examples: llama.cpp server, an adapter around Ollama, etc.)

---

## üîä Piper TTS (Dockerized)

1) **Download voices + configs (host):**
```bash
mkdir -p ~/.local/share/piper/voices
# Amy
curl -L -o ~/.local/share/piper/voices/en_US-amy-medium.onnx \
  "https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx"
curl -L -o ~/.local/share/piper/voices/en_US-amy-medium.onnx.json \
  "https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx.json"

# Kristin
curl -L -o ~/.local/share/piper/voices/en_US-kristin-medium.onnx \
  "https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/kristin/medium/en_US-kristin-medium.onnx"
curl -L -o ~/.local/share/piper/voices/en_US-kristin-medium.onnx.json \
  "https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/kristin/medium/en_US-kristin-medium.onnx.json"

Build & run the container:

```bash

cd tools/assistant/docker/piper
sudo docker build -t piper-tts-jetson .
sudo docker rm -f piper-tts 2>/dev/null || true
sudo docker run --name piper-tts -d \
  -v ~/.local/share/piper/voices:/opt/voices:ro \
  piper-tts-jetson sleep infinity
sudo docker update --restart unless-stopped piper-tts

```
The Dockerfile includes an espeak-ng data fix so phontab is found reliably.

Sanity check:

```bash
sudo docker exec piper-tts bash -lc \
  'echo "Hello from Amy." | /opt/piper/build/piper \
     -m /opt/voices/en_US-amy-medium.onnx \
     -c /opt/voices/en_US-amy-medium.onnx.json \
     --espeak_data /usr/share/espeak-ng-data \
     -f /dev/stdout' | aplay
```

## üó£Ô∏è Python environment

```bash
cd tools/assistant
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
# baseline requirements (Whisper path)
pip install -r requirements.txt
# faster-whisper path (optional)
pip install faster-whisper
```

## üöÄ Run it
One-shot Q‚ÜíA test (fastest path):

```bash
source tools/assistant/.venv/bin/activate
python tools/assistant/ask_once.py 5
```

Continuous assistant (Whisper baseline):

```bash
python tools/assistant/assistant.py
faster-whisper variant (recommended on GPU):
```

```bash
# CPU fallback (works now)
python tools/assistant/assistant_fw.py --device cpu --compute-type int8

# After enabling CUDA in CTranslate2 (see below)
python tools/assistant/assistant_fw.py --device cuda --compute-type float16
```

Wake-word loop (Porcupine‚Äôs ‚Äúcomputer‚Äù):


```bash
pip install pvporcupine pvrecorder
python tools/assistant/wakeword_assistant.py
```

say: "computer" ‚Üí ask a question ‚Üí get a spoken answer

‚ö° Enable GPU for faster-whisper (CTranslate2 with CUDA)

If assistant_fw.py complains about CPU-only CTranslate2, build it with CUDA:

```bash
# deps
sudo apt-get update
sudo apt-get install -y build-essential cmake libopenblas-dev

# in your venv
source tools/assistant/.venv/bin/activate
pip uninstall -y ctranslate2

# build & install (compute 8.7 for Orin)
cd ~
git clone --depth 1 --branch v4.6.0 https://github.com/OpenNMT/CTranslate2.git
cd CTranslate2
cmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DWITH_CUDA=ON -DCUDA_ARCH_LIST="8.7"
cmake --build build -j"$(nproc)"
pip install ./python

# verify
python - <<'PY'
import ctranslate2; print(ctranslate2.get_build_info())
PY
# look for: "cuda": true
```

üîå LLM endpoint
Set LLAMA_URL to your local model server. Examples:

llama.cpp server exposing a /completion-style endpoint on http://127.0.0.1:8080

a small adapter that translates to/from Ollama‚Äôs API

You can export it before running:

```bash
export LLAMA_URL="http://127.0.0.1:8080/completion"
```

## üß≠ Roadmap (kid-friendly upgrades)
- Better wake-word: custom ‚ÄúHey Sophia‚Äù (Porcupine custom / openWakeWord)

- VAD endpointing & barge-in: stop speaking if the child starts talking

- Tutor mode: style guide + guardrails for age-appropriate answers

- LED / face animations: visual cues for wake/listen/speak

- Memory (short-term): simple conversation state, reset on ‚Äúnew topic‚Äù

## üîí Safety & design
Local-first (no cloud) ‚Ä¢ Age-appropriate answers ‚Ä¢ Positive tone

Avoid adult/violent/scary topics by design prompts and filtering

Parents can review/update system prompts

## üìÑ License
MIT ‚Äî see LICENSE.
